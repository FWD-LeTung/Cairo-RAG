import os
from dotenv import load_dotenv
from langchain_deepseek import ChatDeepSeek
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_classic.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage

load_dotenv()
embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
vector_db = Chroma(persist_directory="vector_db/", embedding_function=embeddings)
retriever = vector_db.as_retriever(search_kwargs={"k": 3})

llm = ChatDeepSeek(
    model='deepseek-chat', 
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    temperature=0.1
)

# context prompt
contextualize_q_system_prompt = (
    "Sá»­ dá»¥ng lá»‹ch sá»­ trÃ² chuyá»‡n vÃ  cÃ¢u há»i má»›i nháº¥t cá»§a ngÆ°á»i dÃ¹ng "
    "Ä‘á»ƒ táº¡o ra má»™t cÃ¢u há»i Ä‘á»™c láº­p cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c mÃ  khÃ´ng cáº§n lá»‹ch sá»­."
)
contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)
history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)

#Prompt
system_prompt = (
    "Báº¡n lÃ  chuyÃªn gia tÆ° váº¥n chÆ°Æ¡ng trÃ¬nh ká»¹ sÆ° chuyÃªn sÃ¢u TrÆ°á»ng Äiá»‡n - Äiá»‡n tá»­. "
    "Sá»­ dá»¥ng cÃ¡c Ä‘oáº¡n ngá»¯ cáº£nh sau Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i. "
    "Náº¿u khÃ´ng biáº¿t, hÃ£y nÃ³i khÃ´ng biáº¿t. Tráº£ lá»i chuyÃªn nghiá»‡p báº±ng tiáº¿ng Viá»‡t."
    "\n\n"
    "{context}"
)
qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

# --- BÆ¯á»šC 4: VÃ²ng láº·p Chat liÃªn tá»¥c ---
def start_chat():
    chat_history = [] # NÆ¡i lÆ°u trá»¯ lá»‹ch sá»­ táº¡m thá»i trong phiÃªn lÃ m viá»‡c
    print("\nðŸ¤– BMO: ChÃ o báº¡n! TÃ´i Ä‘Ã£ sáºµn sÃ ng tÆ° váº¥n vá» chÆ°Æ¡ng trÃ¬nh ká»¹ sÆ°. (GÃµ 'exit' Ä‘á»ƒ thoÃ¡t)")
    
    while True:
        user_input = input("\nðŸ‘¤ Báº¡n: ")
        if user_input.lower() in ["exit", "quit", "thoÃ¡t"]:
            print("ðŸ¤– BMO: Táº¡m biá»‡t! Háº¹n gáº·p láº¡i báº¡n.")
            break
            
        if not user_input.strip():
            continue

        response = rag_chain.invoke({
            "input": user_input,
            "chat_history": chat_history
        })

        answer = response["answer"]
        print(f"ðŸ¤– BMO: {answer}")

        chat_history.extend([
            HumanMessage(content=user_input),
            AIMessage(content=answer)
        ])
        
        if len(chat_history) > 10:
            chat_history = chat_history[-10:]

if __name__ == "__main__":
    start_chat()