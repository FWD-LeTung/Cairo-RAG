import os
from dotenv import load_dotenv
from langchain_deepseek import ChatDeepSeek
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_classic.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage


print("Äang khá»Ÿi Ä‘á»™ng há»‡ thá»‘ng BMO...")
load_dotenv()

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    model_kwargs={'device': 'cpu'} 
)

vector_db = Chroma(persist_directory="vector_db/", embedding_function=embeddings)
retriever = vector_db.as_retriever(search_kwargs={"k": 3})

llm = ChatDeepSeek(
    model='deepseek-chat', 
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    temperature=0.1,
    streaming=True 
)

contextualize_q_system_prompt = (
    "Sá»­ dá»¥ng lá»‹ch sá»­ trÃ² chuyá»‡n vÃ  cÃ¢u há»i má»›i nháº¥t cá»§a ngÆ°á»i dÃ¹ng "
    "Ä‘á»ƒ táº¡o ra má»™t cÃ¢u há»i Ä‘á»™c láº­p cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c mÃ  khÃ´ng cáº§n lá»‹ch sá»­."
)
contextualize_q_prompt = ChatPromptTemplate.from_messages([
    ("system", contextualize_q_system_prompt),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])
history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)

system_prompt = (
    "Báº¡n lÃ  chuyÃªn gia tÆ° váº¥n chÆ°Æ¡ng trÃ¬nh ká»¹ sÆ° chuyÃªn sÃ¢u TrÆ°á»ng Äiá»‡n - Äiá»‡n tá»­. "
    "Sá»­ dá»¥ng cÃ¡c Ä‘oáº¡n ngá»¯ cáº£nh sau Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i. "
    "Náº¿u khÃ´ng biáº¿t, hÃ£y nÃ³i khÃ´ng biáº¿t. Tráº£ lá»i chuyÃªn nghiá»‡p báº±ng tiáº¿ng Viá»‡t."
    "\n\n{context}"
)
qa_prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])

question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)



def start_chat():
    chat_history = []
    print("\nHá»‡ thá»‘ng Ä‘Ã£ sáºµn sÃ ng!")
    print("ðŸ¤– BMO: ChÃ o Nuguri! Há»i tÃ´i báº¥t cá»© Ä‘iá»u gÃ¬ vá» chÆ°Æ¡ng trÃ¬nh ká»¹ sÆ°.")
    
    while True:
        user_input = input("\nðŸ‘¤ LeTung: ")
        if user_input.lower() in ["exit", "quit", "thoÃ¡t"]:
            break
            
        print("ðŸ¤– BMO: ", end="", flush=True)
        full_answer = ""

        # Sá»­ dá»¥ng stream Ä‘á»ƒ giáº£m thá»i gian chá» Ä‘á»£i áº£o
        for chunk in rag_chain.stream({"input": user_input, "chat_history": chat_history}):
            if "answer" in chunk:
                print(chunk["answer"], end="", flush=True)
                full_answer += chunk["answer"]
        
        print() # Xuá»‘ng dÃ²ng khi káº¿t thÃºc cÃ¢u tráº£ lá»i

        chat_history.extend([
            HumanMessage(content=user_input),
            AIMessage(content=full_answer)
        ])
        
        if len(chat_history) > 10:
            chat_history = chat_history[-10:]

if __name__ == "__main__":
    start_chat()